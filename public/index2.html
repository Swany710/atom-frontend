<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Atom - Voice Assistant (WAV Converter)</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: #0a0a0a;
            color: #ffffff;
            margin: 0;
            padding: 20px;
            display: flex;
            flex-direction: column;
            align-items: center;
            min-height: 100vh;
        }

        .container {
            max-width: 800px;
            width: 100%;
            background: #1a1a1a;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 10px 30px rgba(0, 255, 157, 0.1);
            border: 1px solid #333;
        }

        h1 {
            color: #00ff9d;
            text-align: center;
            margin-bottom: 30px;
        }

        .voice-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(45deg, #00ff9d, #00bcd4);
            color: white;
            font-size: 1.2em;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 20px auto;
            box-shadow: 0 4px 15px rgba(0, 255, 157, 0.3);
        }

        .voice-button:hover {
            transform: scale(1.1);
            box-shadow: 0 8px 25px rgba(0, 255, 157, 0.5);
        }

        .voice-button.recording {
            background: linear-gradient(45deg, #ff4444, #ff6b6b);
            animation: pulse 1s infinite;
        }

        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }

        .status {
            text-align: center;
            padding: 15px;
            margin: 20px 0;
            border-radius: 10px;
            font-weight: bold;
        }

        .status.success { background: rgba(0, 255, 157, 0.1); color: #00ff9d; }
        .status.error { background: rgba(255, 68, 68, 0.1); color: #ff4444; }
        .status.processing { background: rgba(255, 193, 7, 0.1); color: #ffc107; }
        .status.listening { background: rgba(0, 188, 212, 0.1); color: #00bcd4; }

        .debug-section {
            background: #2a2a2a;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }

        .debug-info {
            background: #333;
            padding: 15px;
            border-radius: 10px;
            margin: 10px 0;
            font-family: monospace;
            font-size: 0.9em;
            color: #ccc;
            max-height: 200px;
            overflow-y: auto;
            white-space: pre-wrap;
        }

        .conversation {
            background: #2a2a2a;
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            min-height: 200px;
            max-height: 400px;
            overflow-y: auto;
        }

        .message {
            margin: 15px 0;
            padding: 12px 18px;
            border-radius: 18px;
            max-width: 80%;
            word-wrap: break-word;
        }

        .message.user {
            background: #00ff9d;
            color: #000;
            margin-left: auto;
            text-align: right;
        }

        .message.assistant {
            background: #333;
            color: #fff;
            margin-right: auto;
        }

        .conversion-info {
            background: #1e3a8a;
            padding: 15px;
            border-radius: 10px;
            margin: 10px 0;
            color: #93c5fd;
            font-family: monospace;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ Atom - Voice Assistant</h1>
        <p style="text-align: center; color: #666;">‚ú® With Audio Format Conversion ‚ú®</p>
        
        <!-- Voice Interface -->
        <button id="voiceBtn" class="voice-button" onclick="toggleRecording()">
            üé§ Start
        </button>

        <div id="status" class="status">Ready for voice commands</div>

        <!-- Conversion Info -->
        <div class="debug-section">
            <h3>üîÑ Audio Conversion Info</h3>
            <div id="conversionInfo" class="conversion-info">Waiting for audio...</div>
        </div>

        <!-- Conversation Display -->
        <div id="conversation" class="conversation"></div>

        <!-- Debug Information -->
        <div class="debug-section">
            <h3>üìù Debug Log</h3>
            <div id="debug" class="debug-info"></div>
        </div>
    </div>

    <script>
        // Backend Configuration
        const BACKEND_URL = 'https://atom-backend-production-8a1e.up.railway.app';
        
        // Voice Recording Variables
        let isRecording = false;
        let mediaRecorder = null;
        let audioChunks = [];

        // UI Elements
        const voiceBtn = document.getElementById('voiceBtn');
        const statusDiv = document.getElementById('status');
        const conversationDiv = document.getElementById('conversation');
        const debugDiv = document.getElementById('debug');
        const conversionInfoDiv = document.getElementById('conversionInfo');

        // Update Status Function
        function updateStatus(message, type = 'info') {
            statusDiv.textContent = message;
            statusDiv.className = `status ${type}`;
            debugLog(`[STATUS ${type.toUpperCase()}] ${message}`);
        }

        // Debug Log Function
        function debugLog(message) {
            const timestamp = new Date().toLocaleTimeString();
            debugDiv.innerHTML += `<div>[${timestamp}] ${message}</div>`;
            debugDiv.scrollTop = debugDiv.scrollHeight;
            console.log(`[DEBUG ${timestamp}] ${message}`);
        }

        // Conversion Info Log
        function conversionLog(message) {
            conversionInfoDiv.textContent = message;
        }

        // Add Message to Conversation
        function addMessage(sender, message) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${sender}`;
            messageDiv.textContent = message;
            conversationDiv.appendChild(messageDiv);
            conversationDiv.scrollTop = conversationDiv.scrollHeight;
        }

        // Voice Recording Functions
        async function toggleRecording() {
            if (!isRecording) {
                await startRecording();
            } else {
                stopRecording();
            }
        }

        async function startRecording() {
            try {
                updateStatus('Requesting microphone access...', 'processing');
                
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 44100
                    }
                });

                debugLog('Microphone access granted');
                conversionLog('Microphone ready - preparing to record...');
                
                audioChunks = [];
                
                // Use WebM format for recording
                let mimeType = 'audio/webm;codecs=opus';
                if (!MediaRecorder.isTypeSupported(mimeType)) {
                    mimeType = 'audio/webm';
                    if (!MediaRecorder.isTypeSupported(mimeType)) {
                        mimeType = ''; // Let browser choose
                    }
                }

                const options = mimeType ? { mimeType } : {};
                mediaRecorder = new MediaRecorder(stream, options);
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                        debugLog(`Audio chunk received: ${event.data.size} bytes`);
                    }
                };
                
                mediaRecorder.onstop = processAudioRecording;
                
                mediaRecorder.start();
                isRecording = true;
                
                voiceBtn.textContent = '‚èπÔ∏è Stop';
                voiceBtn.classList.add('recording');
                updateStatus('üé§ Recording... Speak now!', 'listening');
                conversionLog(`Recording with format: ${mimeType || 'auto'}`);
                
                debugLog(`Recording started with mime type: ${mimeType || 'auto'}`);
                
            } catch (error) {
                updateStatus(`Microphone error: ${error.message}`, 'error');
                debugLog(`Recording error: ${error.message}`);
                conversionLog(`Error: ${error.message}`);
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
            
            if (mediaRecorder?.stream) {
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }
            
            isRecording = false;
            voiceBtn.textContent = 'üé§ Start';
            voiceBtn.classList.remove('recording');
            updateStatus('Processing and converting audio...', 'processing');
            
            debugLog('Recording stopped');
            conversionLog('Recording stopped - preparing for conversion...');
        }

        // WAV Conversion Helper Function
        function audioBufferToWav(buffer) {
            const numOfChannels = buffer.numberOfChannels;
            const sampleRate = buffer.sampleRate;
            const format = 1; // PCM
            const bitDepth = 16;

            const bytesPerSample = bitDepth / 8;
            const blockAlign = numOfChannels * bytesPerSample;

            const samples = [];
            for (let i = 0; i < numOfChannels; i++) {
                samples.push(buffer.getChannelData(i));
            }

            const dataLength = samples[0].length * numOfChannels * bytesPerSample;
            const headerLength = 44;
            const totalLength = headerLength + dataLength;

            const arrayBuffer = new ArrayBuffer(totalLength);
            const view = new DataView(arrayBuffer);

            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            writeString(0, 'RIFF');
            view.setUint32(4, totalLength - 8, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, format, true);
            view.setUint16(22, numOfChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * blockAlign, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bitDepth, true);
            writeString(36, 'data');
            view.setUint32(40, dataLength, true);

            // PCM data
            let offset = headerLength;
            for (let i = 0; i < samples[0].length; i++) {
                for (let channel = 0; channel < numOfChannels; channel++) {
                    const sample = Math.max(-1, Math.min(1, samples[channel][i]));
                    const value = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                    view.setInt16(offset, value, true);
                    offset += 2;
                }
            }

            return new Blob([arrayBuffer], { type: 'audio/wav' });
        }

        // Process Audio Recording with WAV Conversion
        async function processAudioRecording() {
            try {
                debugLog('Processing audio recording...');
                
                if (audioChunks.length === 0) {
                    throw new Error('No audio data recorded');
                }

                const audioBlob = new Blob(audioChunks, { 
                    type: audioChunks[0].type || 'audio/webm' 
                });
                
                debugLog(`Original audio blob: ${audioBlob.size} bytes, type: ${audioBlob.type}`);
                conversionLog(`Original: ${audioBlob.size} bytes (${audioBlob.type})`);
                
                if (audioBlob.size === 0) {
                    throw new Error('Audio recording is empty');
                }

                // CONVERT WEBM TO WAV USING WEB AUDIO API
                updateStatus('Converting WebM to WAV format...', 'processing');
                conversionLog('Converting WebM to WAV format...');
                
                try {
                    // Create audio context
                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    debugLog('Audio context created');
                    
                    // Convert blob to array buffer
                    const arrayBuffer = await audioBlob.arrayBuffer();
                    debugLog(`Array buffer created: ${arrayBuffer.byteLength} bytes`);
                    
                    // Decode audio data
                    conversionLog('Decoding audio data...');
                    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                    debugLog(`Audio decoded: ${audioBuffer.length} samples, ${audioBuffer.sampleRate}Hz, ${audioBuffer.numberOfChannels} channels`);
                    
                    // Convert to WAV format
                    conversionLog('Converting to WAV format...');
                    const wavBlob = audioBufferToWav(audioBuffer);
                    
                    debugLog(`Converted to WAV: ${wavBlob.size} bytes`);
                    conversionLog(`‚úÖ WAV conversion successful: ${wavBlob.size} bytes`);
                    
                    updateStatus('Sending converted audio to backend...', 'processing');
                    
                    // Create FormData with converted WAV
                    const formData = new FormData();
                    formData.append('audio', wavBlob, 'recording.wav');
                    formData.append('userId', 'wav-user');
                    
                    debugLog('Sending WAV to backend...');
                    conversionLog('Uploading WAV to backend...');
                    
                    const response = await fetch(`${BACKEND_URL}/api/v1/ai/voice-command1`, {
                        method: 'POST',
                        body: formData
                    });

                    debugLog(`Backend response: ${response.status} ${response.statusText}`);

                    if (response.ok) {
                        const result = await response.json();
                        debugLog('Backend response received successfully');
                        
                        conversionLog(`‚úÖ Backend processing successful!`);
                        
                        if (result.mode === 'error') {
                            throw new Error(`Backend error: ${result.message}`);
                        }
                        
                        if (result.transcription) {
                            addMessage('user', `üé§ "${result.transcription}"`);
                            debugLog(`Transcription: "${result.transcription}"`);
                        }
                        if (result.message) {
                            addMessage('assistant', result.message);
                        }
                        
                        updateStatus('Voice command processed successfully!', 'success');
                        
                    } else {
                        const errorText = await response.text();
                        conversionLog(`‚ùå Backend error: ${response.status}`);
                        throw new Error(`Backend error: ${response.status} - ${errorText}`);
                    }
                    
                } catch (conversionError) {
                    debugLog(`Audio conversion failed: ${conversionError.message}`);
                    conversionLog(`‚ùå Conversion failed: ${conversionError.message}`);
                    
                    // Fallback to original WebM if conversion fails
                    updateStatus('Conversion failed, trying original format...', 'processing');
                    conversionLog('Trying original WebM format as fallback...');
                    
                    const formData = new FormData();
                    formData.append('audio', audioBlob, 'recording.webm');
                    formData.append('userId', 'webm-user');
                    
                    const response = await fetch(`${BACKEND_URL}/api/v1/ai/voice-command1`, {
                        method: 'POST',
                        body: formData
                    });

                    if (response.ok) {
                        const result = await response.json();
                        
                        if (result.transcription) {
                            addMessage('user', `üé§ "${result.transcription}"`);
                        }
                        if (result.message) {
                            addMessage('assistant', result.message);
                        }
                        
                        updateStatus('Voice command processed with original format!', 'success');
                        conversionLog('‚úÖ Fallback successful with original format');
                    } else {
                        const errorText = await response.text();
                        conversionLog(`‚ùå Both conversion and fallback failed`);
                        throw new Error(`Both conversion and original failed: ${errorText}`);
                    }
                }
                
            } catch (error) {
                updateStatus(`Voice processing failed: ${error.message}`, 'error');
                debugLog(`Processing error: ${error.message}`);
                conversionLog(`‚ùå Processing failed: ${error.message}`);
                addMessage('assistant', `Error: ${error.message}`);
            }
        }

        // Initialize on page load
        document.addEventListener('DOMContentLoaded', function() {
            updateStatus('Voice assistant ready with WAV conversion', 'success');
            debugLog('WAV conversion interface initialized');
            conversionLog('Ready to convert WebM to WAV for Whisper API compatibility');
        });
    </script>
</body>
</html>